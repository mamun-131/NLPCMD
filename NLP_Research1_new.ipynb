{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: single_supporting_fact_10k\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    path = 'C:/Users/310267647/.keras/datasets/babi-tasks-v1-2.tar8.gz'\n",
    "except:\n",
    "  #  print('Error downloading dataset, please download it manually:\\n'\n",
    " #         '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "  #        '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "tar = tarfile.open(path)\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "}\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: Task today is tasktoday . Task for yesterday was taskyesterday .\n",
      "Query: What is task for today ?\n",
      "Answer: tasktoday\n",
      "---\n",
      "Story: Task today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday .\n",
      "Query: What is task for today ?\n",
      "Answer: tasktoday\n",
      "---\n",
      "Story: Task today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow .\n",
      "Query: What is task for today ?\n",
      "Answer: tasktoday\n",
      "---\n",
      "Story: Task today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for tomorrow is tasktomorrow . Task for today is tasktoday .\n",
      "Query: What is task for today ?\n",
      "Answer: tasktoday\n",
      "---\n",
      "Story: Task today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for today is tasktoday . Task for tomorrow is tasktomorrow .\n",
      "Query: What is task for today ?\n",
      "Answer: tasktoday\n",
      "---\n",
      "Story: Task for today is tasktoday . Task for yesterday was taskyesterday .\n",
      "Query: What is task for yesterday ?\n",
      "Answer: taskyesterday\n",
      "---\n",
      "Story: Task for today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday .\n",
      "Query: What is task for yesterday ?\n",
      "Answer: taskyesterday\n",
      "---\n",
      "Story: Task for today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for yesterday was taskyesterday . task for tomorrow is tasktomorrow .\n",
      "Query: What is task for yesterday ?\n",
      "Answer: taskyesterday\n",
      "---\n",
      "Story: Task for today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for yesterday was taskyesterday . task for tomorrow is tasktomorrow . Task for tomorrow is tasktomorrow . Task for today is tasktoday .\n",
      "Query: What is task for yesterday ?\n",
      "Answer: taskyesterday\n",
      "---\n",
      "Story: Task for today is tasktoday . Task for yesterday was taskyesterday . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for yesterday was taskyesterday . task for tomorrow is tasktomorrow . Task for tomorrow is tasktomorrow . Task for today is tasktoday . Task for today is tasktoday . Task for tomorrow is tasktomorrow .\n",
      "Query: What is task for yesterday ?\n",
      "Answer: taskyesterday\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# See what the data looks like# See w \n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Story: {}\".format(' '.join(train_stories[i][0])))\n",
    "    print(\"Query: {}\".format(' '.join(train_stories[i][1])))\n",
    "    print(\"Answer: {}\".format(train_stories[i][2]))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Vocab size: 15 unique words\n",
      "Story max length: 60 words\n",
      "Query max length: 6 words\n",
      "Number of training stories: 10\n",
      "Number of test stories: 10\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['Task', 'today', 'is', 'tasktoday', '.', 'Task', 'for', 'yesterday', 'was', 'taskyesterday', '.'], ['What', 'is', 'task', 'for', 'today', '?'], 'tasktoday')\n",
      "-\n",
      "(0, '.')\n",
      "(1, '?')\n",
      "(2, 'Task')\n",
      "(3, 'What')\n",
      "(4, 'for')\n",
      "(5, 'is')\n",
      "(6, 'task')\n",
      "(7, 'tasktoday')\n",
      "(8, 'tasktomorrow')\n",
      "(9, 'taskyesterday')\n",
      "(10, 'today')\n",
      "(11, 'tomorrow')\n",
      "(12, 'was')\n",
      "(13, 'yesterday')\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "\n",
    "\n",
    "for s in list(enumerate(vocab)):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (10, 60)\n",
      "inputs_test shape: (10, 60)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (10, 6)\n",
      "queries_test shape: (10, 6)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (10,)\n",
      "answers_test shape: (10,)\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing the word sequences...')\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story (x): [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  3 11  6  8  1  3  5 14 13 10  1]\n",
      "Question (x): [ 4  6  7  5 11  2]\n",
      "Answer: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# See individual training element.# See i \n",
    "\n",
    "print(\"Story (x): {}\".format(inputs_train[0]))\n",
    "print(\"Question (x): {}\".format(queries_train[0]))\n",
    "print(\"Answer: {}\".format(answers_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Compiling...')\n",
    "\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 10 samples\n",
      "Epoch 1/120\n",
      "10/10 [==============================] - 0s - loss: 0.9063 - acc: 0.6000 - val_loss: 0.8517 - val_acc: 0.5000\n",
      "Epoch 2/120\n",
      "10/10 [==============================] - 0s - loss: 0.7695 - acc: 0.8000 - val_loss: 0.8333 - val_acc: 0.5000\n",
      "Epoch 3/120\n",
      "10/10 [==============================] - 0s - loss: 0.8672 - acc: 0.7000 - val_loss: 0.8180 - val_acc: 0.5000\n",
      "Epoch 4/120\n",
      "10/10 [==============================] - 0s - loss: 0.8827 - acc: 0.4000 - val_loss: 0.8111 - val_acc: 0.5000\n",
      "Epoch 5/120\n",
      "10/10 [==============================] - 0s - loss: 0.8824 - acc: 0.4000 - val_loss: 0.8003 - val_acc: 0.5000\n",
      "Epoch 6/120\n",
      "10/10 [==============================] - 0s - loss: 0.7524 - acc: 0.6000 - val_loss: 0.7871 - val_acc: 0.5000\n",
      "Epoch 7/120\n",
      "10/10 [==============================] - 0s - loss: 1.1520 - acc: 0.2000 - val_loss: 0.7803 - val_acc: 0.5000\n",
      "Epoch 8/120\n",
      "10/10 [==============================] - 0s - loss: 0.6867 - acc: 0.7000 - val_loss: 0.7667 - val_acc: 0.5000\n",
      "Epoch 9/120\n",
      "10/10 [==============================] - 0s - loss: 0.8018 - acc: 0.6000 - val_loss: 0.7582 - val_acc: 0.5000\n",
      "Epoch 10/120\n",
      "10/10 [==============================] - 0s - loss: 0.7229 - acc: 0.5000 - val_loss: 0.7497 - val_acc: 0.5000\n",
      "Epoch 11/120\n",
      "10/10 [==============================] - 0s - loss: 0.7575 - acc: 0.8000 - val_loss: 0.7426 - val_acc: 0.5000\n",
      "Epoch 12/120\n",
      "10/10 [==============================] - 0s - loss: 0.7443 - acc: 0.7000 - val_loss: 0.7363 - val_acc: 0.7000\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - 0s - loss: 0.9650 - acc: 0.5000 - val_loss: 0.7300 - val_acc: 0.7000\n",
      "Epoch 14/120\n",
      "10/10 [==============================] - 0s - loss: 0.8571 - acc: 0.5000 - val_loss: 0.7246 - val_acc: 0.5000\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - 0s - loss: 0.7178 - acc: 0.5000 - val_loss: 0.7178 - val_acc: 0.7000\n",
      "Epoch 16/120\n",
      "10/10 [==============================] - 0s - loss: 0.9066 - acc: 0.2000 - val_loss: 0.7103 - val_acc: 0.6000\n",
      "Epoch 17/120\n",
      "10/10 [==============================] - 0s - loss: 0.7929 - acc: 0.6000 - val_loss: 0.7028 - val_acc: 0.9000\n",
      "Epoch 18/120\n",
      "10/10 [==============================] - 0s - loss: 0.6754 - acc: 0.9000 - val_loss: 0.6946 - val_acc: 0.9000\n",
      "Epoch 19/120\n",
      "10/10 [==============================] - 0s - loss: 0.7612 - acc: 0.6000 - val_loss: 0.6872 - val_acc: 0.8000\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - 0s - loss: 0.6983 - acc: 0.6000 - val_loss: 0.6802 - val_acc: 1.0000\n",
      "Epoch 21/120\n",
      "10/10 [==============================] - 0s - loss: 0.7096 - acc: 0.7000 - val_loss: 0.6728 - val_acc: 0.5000\n",
      "Epoch 22/120\n",
      "10/10 [==============================] - 0s - loss: 0.7989 - acc: 0.4000 - val_loss: 0.6615 - val_acc: 1.0000\n",
      "Epoch 23/120\n",
      "10/10 [==============================] - 0s - loss: 0.8173 - acc: 0.5000 - val_loss: 0.6583 - val_acc: 1.0000\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - 0s - loss: 0.7531 - acc: 0.5000 - val_loss: 0.6487 - val_acc: 1.0000\n",
      "Epoch 25/120\n",
      "10/10 [==============================] - 0s - loss: 0.8472 - acc: 0.6000 - val_loss: 0.6386 - val_acc: 1.0000\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - 0s - loss: 0.7980 - acc: 0.5000 - val_loss: 0.6298 - val_acc: 1.0000\n",
      "Epoch 27/120\n",
      "10/10 [==============================] - 0s - loss: 0.6098 - acc: 0.9000 - val_loss: 0.6227 - val_acc: 1.0000\n",
      "Epoch 28/120\n",
      "10/10 [==============================] - 0s - loss: 0.5611 - acc: 0.8000 - val_loss: 0.6135 - val_acc: 1.0000\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - 0s - loss: 0.6350 - acc: 0.7000 - val_loss: 0.6030 - val_acc: 1.0000\n",
      "Epoch 30/120\n",
      "10/10 [==============================] - 0s - loss: 0.5719 - acc: 0.7000 - val_loss: 0.5955 - val_acc: 1.0000\n",
      "Epoch 31/120\n",
      "10/10 [==============================] - 0s - loss: 0.7034 - acc: 0.5000 - val_loss: 0.5770 - val_acc: 1.0000\n",
      "Epoch 32/120\n",
      "10/10 [==============================] - 0s - loss: 0.6397 - acc: 0.7000 - val_loss: 0.5744 - val_acc: 1.0000\n",
      "Epoch 33/120\n",
      "10/10 [==============================] - 0s - loss: 0.7050 - acc: 0.6000 - val_loss: 0.5566 - val_acc: 1.0000\n",
      "Epoch 34/120\n",
      "10/10 [==============================] - 0s - loss: 0.5136 - acc: 0.9000 - val_loss: 0.5443 - val_acc: 1.0000\n",
      "Epoch 35/120\n",
      "10/10 [==============================] - 0s - loss: 0.5410 - acc: 0.8000 - val_loss: 0.5259 - val_acc: 1.0000\n",
      "Epoch 36/120\n",
      "10/10 [==============================] - 0s - loss: 0.5583 - acc: 0.9000 - val_loss: 0.5123 - val_acc: 1.0000\n",
      "Epoch 37/120\n",
      "10/10 [==============================] - 0s - loss: 0.4890 - acc: 0.8000 - val_loss: 0.4967 - val_acc: 1.0000\n",
      "Epoch 38/120\n",
      "10/10 [==============================] - 0s - loss: 0.4437 - acc: 1.0000 - val_loss: 0.4798 - val_acc: 1.0000\n",
      "Epoch 39/120\n",
      "10/10 [==============================] - 0s - loss: 0.5012 - acc: 0.9000 - val_loss: 0.4690 - val_acc: 1.0000\n",
      "Epoch 40/120\n",
      "10/10 [==============================] - 0s - loss: 0.5061 - acc: 0.8000 - val_loss: 0.4519 - val_acc: 1.0000\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - 0s - loss: 0.5830 - acc: 0.7000 - val_loss: 0.4384 - val_acc: 1.0000\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - 0s - loss: 0.7153 - acc: 0.7000 - val_loss: 0.4220 - val_acc: 1.0000\n",
      "Epoch 43/120\n",
      "10/10 [==============================] - 0s - loss: 0.4053 - acc: 1.0000 - val_loss: 0.4194 - val_acc: 1.0000\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - 0s - loss: 0.4882 - acc: 0.8000 - val_loss: 0.3905 - val_acc: 1.0000\n",
      "Epoch 45/120\n",
      "10/10 [==============================] - 0s - loss: 0.4311 - acc: 0.8000 - val_loss: 0.3719 - val_acc: 1.0000\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - 0s - loss: 0.4141 - acc: 1.0000 - val_loss: 0.3531 - val_acc: 1.0000\n",
      "Epoch 47/120\n",
      "10/10 [==============================] - 0s - loss: 0.5444 - acc: 0.8000 - val_loss: 0.3439 - val_acc: 1.0000\n",
      "Epoch 48/120\n",
      "10/10 [==============================] - 0s - loss: 0.3563 - acc: 1.0000 - val_loss: 0.3206 - val_acc: 1.0000\n",
      "Epoch 49/120\n",
      "10/10 [==============================] - 0s - loss: 0.4171 - acc: 1.0000 - val_loss: 0.3066 - val_acc: 1.0000\n",
      "Epoch 50/120\n",
      "10/10 [==============================] - 0s - loss: 0.5241 - acc: 0.9000 - val_loss: 0.2995 - val_acc: 1.0000\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - 0s - loss: 0.3114 - acc: 0.9000 - val_loss: 0.2817 - val_acc: 1.0000\n",
      "Epoch 52/120\n",
      "10/10 [==============================] - 0s - loss: 0.3765 - acc: 1.0000 - val_loss: 0.2617 - val_acc: 1.0000\n",
      "Epoch 53/120\n",
      "10/10 [==============================] - 0s - loss: 0.2770 - acc: 1.0000 - val_loss: 0.2429 - val_acc: 1.0000\n",
      "Epoch 54/120\n",
      "10/10 [==============================] - 0s - loss: 0.2583 - acc: 1.0000 - val_loss: 0.2339 - val_acc: 1.0000\n",
      "Epoch 55/120\n",
      "10/10 [==============================] - 0s - loss: 0.3620 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 1.0000\n",
      "Epoch 56/120\n",
      "10/10 [==============================] - 0s - loss: 0.3005 - acc: 1.0000 - val_loss: 0.2062 - val_acc: 1.0000\n",
      "Epoch 57/120\n",
      "10/10 [==============================] - 0s - loss: 0.2746 - acc: 1.0000 - val_loss: 0.2080 - val_acc: 1.0000\n",
      "Epoch 58/120\n",
      "10/10 [==============================] - 0s - loss: 0.1906 - acc: 1.0000 - val_loss: 0.1885 - val_acc: 1.0000\n",
      "Epoch 59/120\n",
      "10/10 [==============================] - 0s - loss: 0.2405 - acc: 1.0000 - val_loss: 0.1947 - val_acc: 1.0000\n",
      "Epoch 60/120\n",
      "10/10 [==============================] - 0s - loss: 0.2812 - acc: 0.9000 - val_loss: 0.2111 - val_acc: 1.0000\n",
      "Epoch 61/120\n",
      "10/10 [==============================] - 0s - loss: 0.1753 - acc: 1.0000 - val_loss: 0.1645 - val_acc: 1.0000\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - 0s - loss: 0.2179 - acc: 1.0000 - val_loss: 0.1652 - val_acc: 1.0000\n",
      "Epoch 63/120\n",
      "10/10 [==============================] - 0s - loss: 0.2907 - acc: 1.0000 - val_loss: 0.1401 - val_acc: 1.0000\n",
      "Epoch 64/120\n",
      "10/10 [==============================] - 0s - loss: 0.1519 - acc: 1.0000 - val_loss: 0.1320 - val_acc: 1.0000\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - 0s - loss: 0.1204 - acc: 1.0000 - val_loss: 0.1295 - val_acc: 1.0000\n",
      "Epoch 66/120\n",
      "10/10 [==============================] - 0s - loss: 0.2223 - acc: 1.0000 - val_loss: 0.1179 - val_acc: 1.0000\n",
      "Epoch 67/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s - loss: 0.2022 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 1.0000\n",
      "Epoch 68/120\n",
      "10/10 [==============================] - 0s - loss: 0.1148 - acc: 1.0000 - val_loss: 0.1090 - val_acc: 1.0000\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - 0s - loss: 0.1535 - acc: 1.0000 - val_loss: 0.1070 - val_acc: 1.0000\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - 0s - loss: 0.1502 - acc: 1.0000 - val_loss: 0.1062 - val_acc: 1.0000\n",
      "Epoch 71/120\n",
      "10/10 [==============================] - 0s - loss: 0.0958 - acc: 1.0000 - val_loss: 0.0935 - val_acc: 1.0000\n",
      "Epoch 72/120\n",
      "10/10 [==============================] - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 0.0875 - val_acc: 1.0000\n",
      "Epoch 73/120\n",
      "10/10 [==============================] - 0s - loss: 0.0957 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 1.0000\n",
      "Epoch 74/120\n",
      "10/10 [==============================] - 0s - loss: 0.0988 - acc: 1.0000 - val_loss: 0.0786 - val_acc: 1.0000\n",
      "Epoch 75/120\n",
      "10/10 [==============================] - 0s - loss: 0.0905 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 1.0000\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - 0s - loss: 0.0885 - acc: 1.0000 - val_loss: 0.0712 - val_acc: 1.0000\n",
      "Epoch 77/120\n",
      "10/10 [==============================] - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.0678 - val_acc: 1.0000\n",
      "Epoch 78/120\n",
      "10/10 [==============================] - 0s - loss: 0.0858 - acc: 1.0000 - val_loss: 0.0641 - val_acc: 1.0000\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - 0s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 1.0000\n",
      "Epoch 80/120\n",
      "10/10 [==============================] - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 1.0000\n",
      "Epoch 81/120\n",
      "10/10 [==============================] - 0s - loss: 0.0771 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 1.0000\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 1.0000\n",
      "Epoch 83/120\n",
      "10/10 [==============================] - 0s - loss: 0.1188 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 1.0000\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 1.0000\n",
      "Epoch 85/120\n",
      "10/10 [==============================] - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 1.0000\n",
      "Epoch 86/120\n",
      "10/10 [==============================] - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - 0s - loss: 0.0870 - acc: 1.0000 - val_loss: 0.0397 - val_acc: 1.0000\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - 0s - loss: 0.0543 - acc: 1.0000 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 89/120\n",
      "10/10 [==============================] - 0s - loss: 0.0543 - acc: 1.0000 - val_loss: 0.0396 - val_acc: 1.0000\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - 0s - loss: 0.0430 - acc: 1.0000 - val_loss: 0.0345 - val_acc: 1.0000\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.0329 - val_acc: 1.0000\n",
      "Epoch 92/120\n",
      "10/10 [==============================] - 0s - loss: 0.0878 - acc: 1.0000 - val_loss: 0.0362 - val_acc: 1.0000\n",
      "Epoch 93/120\n",
      "10/10 [==============================] - 0s - loss: 0.0658 - acc: 1.0000 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "Epoch 94/120\n",
      "10/10 [==============================] - 0s - loss: 0.0539 - acc: 1.0000 - val_loss: 0.0302 - val_acc: 1.0000\n",
      "Epoch 95/120\n",
      "10/10 [==============================] - 0s - loss: 0.0729 - acc: 1.0000 - val_loss: 0.0306 - val_acc: 1.0000\n",
      "Epoch 96/120\n",
      "10/10 [==============================] - 0s - loss: 0.0427 - acc: 1.0000 - val_loss: 0.0266 - val_acc: 1.0000\n",
      "Epoch 97/120\n",
      "10/10 [==============================] - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "Epoch 98/120\n",
      "10/10 [==============================] - 0s - loss: 0.0590 - acc: 1.0000 - val_loss: 0.0233 - val_acc: 1.0000\n",
      "Epoch 99/120\n",
      "10/10 [==============================] - 0s - loss: 0.0464 - acc: 1.0000 - val_loss: 0.0224 - val_acc: 1.0000\n",
      "Epoch 100/120\n",
      "10/10 [==============================] - 0s - loss: 0.0378 - acc: 1.0000 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "Epoch 101/120\n",
      "10/10 [==============================] - 0s - loss: 0.0301 - acc: 1.0000 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 102/120\n",
      "10/10 [==============================] - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 0.0214 - val_acc: 1.0000\n",
      "Epoch 103/120\n",
      "10/10 [==============================] - 0s - loss: 0.0491 - acc: 1.0000 - val_loss: 0.0217 - val_acc: 1.0000\n",
      "Epoch 104/120\n",
      "10/10 [==============================] - 0s - loss: 0.0233 - acc: 1.0000 - val_loss: 0.0185 - val_acc: 1.0000\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - 0s - loss: 0.0230 - acc: 1.0000 - val_loss: 0.0187 - val_acc: 1.0000\n",
      "Epoch 106/120\n",
      "10/10 [==============================] - 0s - loss: 0.0184 - acc: 1.0000 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - 0s - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - 0s - loss: 0.0363 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "Epoch 109/120\n",
      "10/10 [==============================] - 0s - loss: 0.0329 - acc: 1.0000 - val_loss: 0.0180 - val_acc: 1.0000\n",
      "Epoch 110/120\n",
      "10/10 [==============================] - 0s - loss: 0.0333 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 1.0000\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - 0s - loss: 0.0473 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 1.0000\n",
      "Epoch 112/120\n",
      "10/10 [==============================] - 0s - loss: 0.0290 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 113/120\n",
      "10/10 [==============================] - 0s - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - 0s - loss: 0.0348 - acc: 1.0000 - val_loss: 0.0166 - val_acc: 1.0000\n",
      "Epoch 115/120\n",
      "10/10 [==============================] - 0s - loss: 0.0424 - acc: 1.0000 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 116/120\n",
      "10/10 [==============================] - 0s - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 1.0000\n",
      "Epoch 117/120\n",
      "10/10 [==============================] - 0s - loss: 0.0259 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 1.0000\n",
      "Epoch 118/120\n",
      "10/10 [==============================] - 0s - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - 0s - loss: 0.0283 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 1.0000\n",
      "Epoch 120/120\n",
      "10/10 [==============================] - 0s - loss: 0.0293 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# train\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=120,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "# save\n",
    "save_path = \"C:/Users/310267647/.keras/datasets/\"\n",
    "# save entire network to HDF5 (save everything, suggested)\n",
    "model.save(os.path.join(save_path,\"chatbot.h5\"))\n",
    "# save the vocab too, indexes must be the same\n",
    "pickle.dump( vocab, open( os.path.join(save_path,\"vocab.pkl\"), \"wb\" ) )\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "#print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model, if it exists, load vocab too\n",
    "save_path = \"C:/Users/310267647/.keras/datasets/\"\n",
    "model = load_model(os.path.join(save_path,\"chatbot.h5\"))\n",
    "vocab = pickle.load( open( os.path.join(save_path,\"vocab.pkl\"), \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46445323e-04 2.29949830e-04 1.37360534e-04 2.65870505e-04\n",
      "  3.72650393e-04 1.59459276e-04 7.08726075e-05 1.74221364e-04\n",
      "  9.83692050e-01 1.03552057e-03 1.21412994e-02 4.64760466e-04\n",
      "  5.36303967e-04 3.34551878e-04 2.38698980e-04]\n",
      " [1.47957267e-04 2.36868596e-04 1.38159754e-04 2.74190475e-04\n",
      "  3.72426701e-04 1.58981915e-04 6.92500180e-05 1.81049007e-04\n",
      "  9.84245121e-01 1.04032014e-03 1.15229189e-02 4.75112174e-04\n",
      "  5.45805611e-04 3.49255773e-04 2.42442766e-04]\n",
      " [1.50757987e-04 2.38692272e-04 1.39892640e-04 2.74852879e-04\n",
      "  3.78800265e-04 1.60344571e-04 7.09761298e-05 1.81664931e-04\n",
      "  9.84003127e-01 1.06529752e-03 1.17036225e-02 4.79606184e-04\n",
      "  5.53729304e-04 3.54458723e-04 2.43959890e-04]\n",
      " [1.55803282e-04 2.47488177e-04 1.43719677e-04 2.77600542e-04\n",
      "  3.82754981e-04 1.71333610e-04 7.15834030e-05 1.86043675e-04\n",
      "  9.84421253e-01 1.08853995e-03 1.11796940e-02 4.78396309e-04\n",
      "  5.69860218e-04 3.73542396e-04 2.52296770e-04]\n",
      " [1.57840157e-04 2.43616494e-04 1.43025711e-04 2.76496547e-04\n",
      "  3.87537439e-04 1.70934669e-04 7.11212706e-05 1.87378871e-04\n",
      "  9.84415233e-01 1.07333960e-03 1.11955218e-02 4.80343180e-04\n",
      "  5.70419710e-04 3.71955393e-04 2.55131192e-04]\n",
      " [3.21306834e-05 1.72057553e-04 2.01451021e-05 1.53833447e-04\n",
      "  6.40697399e-05 1.13628354e-04 1.74684421e-04 4.70384584e-05\n",
      "  5.70172910e-03 7.55360961e-05 9.93086219e-01 1.59645409e-04\n",
      "  7.40569449e-05 6.59775033e-05 5.91758217e-05]\n",
      " [3.26682566e-05 1.74526795e-04 2.01187304e-05 1.57016373e-04\n",
      "  6.39428617e-05 1.15309311e-04 1.74657165e-04 4.84032316e-05\n",
      "  5.73106064e-03 7.68276514e-05 9.93041158e-01 1.60484880e-04\n",
      "  7.53326167e-05 6.85317573e-05 5.99537234e-05]\n",
      " [3.13235869e-05 1.71783962e-04 1.99629230e-05 1.54438239e-04\n",
      "  6.33491727e-05 1.11217523e-04 1.67038830e-04 4.76357200e-05\n",
      "  5.51643083e-03 7.45958823e-05 9.93286312e-01 1.59194955e-04\n",
      "  7.36069633e-05 6.63251994e-05 5.68751493e-05]\n",
      " [3.36291378e-05 1.73554712e-04 1.99904389e-05 1.55302303e-04\n",
      "  6.31298462e-05 1.16870557e-04 1.75081630e-04 4.91389510e-05\n",
      "  5.57788601e-03 7.59898976e-05 9.93198574e-01 1.58420953e-04\n",
      "  7.28185114e-05 6.93054244e-05 6.03345325e-05]\n",
      " [3.31732372e-05 1.79350420e-04 2.00863433e-05 1.62394237e-04\n",
      "  6.49403810e-05 1.17612028e-04 1.78465256e-04 5.12440638e-05\n",
      "  5.41242119e-03 8.00008493e-05 9.93330657e-01 1.58326467e-04\n",
      "  7.77672758e-05 7.16074646e-05 6.18746562e-05]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([inputs_test, queries_test])\n",
    "# See what the predictions look like, they are just probabilities of each class.\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  8  8  8  8 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "# Use argmax to turn those into actual predictions.  The class (word) with the highest\n",
    "# probability is the answer.\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(answers_test, pred)\n",
    "print(\"Final accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember, I only know these words: ['.', '?', 'Task', 'What', 'for', 'is', 'task', 'tasktoday', 'tasktomorrow', 'taskyesterday', 'today', 'tomorrow', 'was', 'yesterday']\n",
      "\n",
      "[3.1849337e-05 1.7253027e-04 1.9919404e-05 1.5431790e-04 6.3069027e-05\n",
      " 1.1277236e-04 1.7380287e-04 4.7067351e-05 5.5776616e-03 7.4987591e-05\n",
      " 9.9321401e-01 1.5879613e-04 7.4054755e-05 6.6307461e-05 5.8981997e-05]\n",
      "Answer: taskyesterday([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Remember, I only know these words: {}\".format(vocab))\n",
    "print()\n",
    "story = \"Task today is tasktoday. Task for tomorrow is tasktomorrow. Task for yesterday was taskyesterday.\"\n",
    "query = \"What is task for yesterday?\"\n",
    "\n",
    "adhoc_stories = (tokenize(story), tokenize(query), '?')\n",
    "\n",
    "adhoc_train, adhoc_query, adhoc_answer = vectorize_stories([adhoc_stories])\n",
    "\n",
    "pred = model.predict([adhoc_train, adhoc_query])\n",
    "print(pred[0])\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(\"Answer: {}({})\".format(vocab[pred[0]-1],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
